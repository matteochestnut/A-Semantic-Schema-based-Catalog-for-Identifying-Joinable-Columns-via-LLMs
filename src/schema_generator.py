import os
import json
import yaml
import pandas as pd
from tqdm import tqdm
from collections import defaultdict
from openai import OpenAI
from typing import Union

from utils import add_timestamp, color_text, create_directory_if_not_exists, load_datalake_json, yaml_parser

class SchemaGenerator:
    """
    Generates a LinkML schema from a processed data lake representation.
    It collects column data, unifies descriptions for common columns,
    and then generates the LinkML YAML schema using an LLM.
    """

    def __init__(self,
        openai_client: OpenAI
    ):
        """
        Initializes the SchemaGenerator with an OpenAI client.

        Args:
            openai_client (OpenAI): An initialized OpenAI client instance.
        """
        self.openai_client = openai_client

    def _extract_classes_from_columns(self,
        columns
    ) -> dict:
        """
        Extracts classes and their attributes from a list of column names.

        This method processes a list of column names, identifying classes based on
        the format `class.attribute`. If a column name follows this format, the
        attribute is added to the corresponding class. If a column name does not
        have a class prefix, it is added to a "Document" class.

        Args:
            columns (list of str): The list of column names to process.

        Returns:
            dict: A dictionary where keys are class names and values are lists of attributes.
        """
        classes = defaultdict(list)
        
        for column in columns:
            parts = column.split('.')
            if len(parts) > 1:
                # If format is class.attribute, add to that class
                class_name = parts[0]
                attribute = '.'.join(parts[1:])
                classes[class_name].append(attribute)
            else:
                # If no class prefix, add to a general Document class
                classes["Document"].append(column)
        
        return classes

    def generate_linkml_schema(self,
        data_lake_list: Union[list[dict], str],
        output_directory: str | None = None
    ) -> dict:
        """
        Generates a LinkML schema from a processed data lake representation.
        
        This method collects column data, unifies descriptions for common columns,
        and then generates the LinkML YAML schema using an LLM.
        
        Args:
            data_lake_list (list[dict] | str): The list of dictionaries representing the data lake,
                                            as generated by DataLoader.load_and_describe_datalake.
            output_directory (str | None): If specified, the generated LinkML schema will be saved to a file
                                        in this directory, named "linkml_schema.yaml".
        
        Returns:
            dict | None: The generated LinkML schema in dictionary format, or None if an error occurs.
        """
        if not data_lake_list:
            print(add_timestamp(color_text("No data lake information provided to generate schema.", "yellow")))
            return None
        
        if isinstance(data_lake_list, str):
            if not os.path.exists(data_lake_list):
                print(add_timestamp(color_text(f"Error: Data lake JSON file not found at {data_lake_list}", "red")))
            data_lake_list = load_datalake_json(data_lake_list)

        column_names = []
        all_columns = defaultdict(lambda: {"description": [], "examples": set()})
        for file_info in data_lake_list:
            for col_dict in file_info['columns']:
                col_name = col_dict['column_name']
                column_names.append(col_name)
                all_columns[col_name]["description"] = col_dict.get('llm_description', '')
                all_columns[col_name]["examples"] = col_dict.get('values_sample', [])
        column_names = set(column_names)
        classes = self._extract_classes_from_columns(column_names)

        # --- Generate LinkML Schema using LLM ---
        prompt = f"""Create a LinkML schema for a Eurostat dataset with a single "Document" class that inherits from "NamedEntity".

IDENTIFIED CLASSES AND THEIR ATTRIBUTES:
"""
        for class_name, attributes in classes.items():
            prompt += f"\nCLASS: {class_name}\nATTRIBUTES: {', '.join(attributes)}\n"
        
        prompt += "\nDETAILED COLUMN DATA (description and examples):\n"
        for column, data in all_columns.items():
            description = data["description"]
            examples = data["examples"]
            prompt += f"\n{column}:\n  Description: {description}\n  Examples: {', '.join(examples)}"
        
        prompt += """

TASK:
Create a comprehensive LinkML schema that includes all identified classes and their attributes.

RULES:
1. Follow the LinkML schema format as shown in the example below
2. Create a class for each identified class in the data
3. All classes should inherit from "NamedEntity"
4. For attributes that don't have a class prefix (like 'attribute' vs 'class.attribute'), 
   add them to a "Document" class that also inherits from "NamedEntity"
5. Use appropriate data types for each attribute based on the examples provided (string, integer, float, etc.)
6. Include clear descriptions for each attribute
7. Include examples for each attribute using the provided examples
8. Group related attributes together in the YAML structure
9. Use standard LinkML prefixes and imports

EXAMPLE FORMAT:
```yaml
id: https://w3id.org/schema
name: dataset-schema
title: Dataset Schema
description: >-
  A schema for a dataset.
prefixes:
  linkml: https://w3id.org/linkml/
  core: http://w3id.org/ontogpt/core/

default_range: string

imports:
  - linkml:types
  - core

classes:
  Document:
    is_a: NamedEntity
    description: A document representing a dataset
    attributes:
      attribute1:
        description: Description of attribute1
        range: string
        examples: example1, example2
  
  Gene:
    is_a: NamedEntity
    description: A gene entity
    attributes:
      type:
        description: The type of gene
        range: string
        examples: protein-coding, ncRNA
      symbol:
        description: The gene symbol
        range: string
        examples: BRCA1, TP53
```

RESPONSE FORMAT:
{
    "schema": "Your complete LinkML schema in YAML format here (without the ```yaml and ``` markers)"
}
"""
        print(add_timestamp(color_text("Generating LinkML schema with LLM...", "yellow")))
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o", # Can be made configurable
                messages=[
                    {"role": "system", "content": "You are a data modeling expert specializing in LinkML schemas. You create well-structured, comprehensive schemas following LinkML conventions. Always respond in JSON format."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                response_format={"type": "json_object"}
            )

            result = json.loads(response.choices[0].message.content)
            schema_yaml = result.get("schema", "")

            # Basic validation and correction for 'is_a: NamedEntity'
            try:
                yaml_data = yaml.safe_load(schema_yaml)
                if "classes" not in yaml_data or "Document" not in yaml_data["classes"]:
                    print(add_timestamp(color_text("Warning: Schema does not contain a 'Document' class. Attempting to add it.", "yellow")))
                    if "classes" not in yaml_data:
                        yaml_data["classes"] = {}
                    yaml_data["classes"]["Document"] = {"is_a": "NamedEntity", "description": "A document representing a dataset"}
                    # If attributes were generated but not under Document, try to move them
                    if "attributes" in yaml_data and "Document" not in yaml_data["classes"]:
                        yaml_data["classes"]["Document"]["attributes"] = yaml_data.pop("attributes")
                elif "is_a" not in yaml_data["classes"]["Document"] or yaml_data["classes"]["Document"]["is_a"] != "NamedEntity":
                    print(add_timestamp(color_text("Warning: Correcting 'Document' class 'is_a' to 'NamedEntity'.", "yellow")))
                    yaml_data["classes"]["Document"]["is_a"] = "NamedEntity"

                schema_yaml = yaml.dump(yaml_data, default_flow_style=False, sort_keys=False, indent=2)
            except Exception as e:
                print(add_timestamp(color_text(f"Warning: Could not validate or correct schema structure: {e}", "red")))

            print(add_timestamp(color_text("Successfully generated LinkML schema.", "green")))
            
            if output_directory:
                try:
                    create_directory_if_not_exists(output_directory)
                    output_file = os.path.join(output_directory, "linkml_schema.yaml")
                    with open(output_file, 'w', encoding='utf-8') as f:
                        f.write(schema_yaml)
                    print(add_timestamp(color_text(f"LinkML schema saved to: {output_file}", "green")))
                except Exception as e:
                    print(add_timestamp(color_text(f"Error saving LinkML schema: {e}", "red")))
            
            return yaml_data

        except Exception as e:
            print(add_timestamp(color_text(f"Error generating LinkML schema: {e}", "red")))
            return None

    def _classify_document_classes(self,
        file_info: dict,
        schema_descriptions: dict
    ) -> dict | None:
        """
        Creates a prompt and gets classification from LLM to identify the most relevant
        schema classes for the entire document.

        Args:
            file_info (dict): A dictionary representing a single file from the data lake list.
            schema_descriptions (dict): Schema class descriptions (only top-level classes).

        Returns:
            dict | None: Classification results, or None if an error occurs.
        """
        prompt = f"""Analyze this dataset as a whole document and identify which schema classes (maximum 3) best characterize its content and purpose.

FILE INFORMATION:
- File name: {file_info['file_name']}"""

        if file_info['original_headers']:
            prompt += f"\n- Headers: {', '.join(file_info['original_headers'])}"

        prompt += "\n\nCOLUMN SAMPLES (first 5 samples per column):"

        for col_dict in file_info['columns']:
            # Use values_sample from the column dict
            prompt += f"\n{col_dict['column_name']}: {col_dict['values_sample'][:5]}"

        prompt += "\n\nSCHEMA CLASSES:"
        for class_name, description in schema_descriptions.items():
            prompt += f"\n{class_name}: {description}"

        prompt += """

TASK:
Identify the most relevant schema classes (maximum 3) that best characterize this document as a whole.

RESPONSE FORMAT:
{
    "file_name": "example.tsv",
    "relevant_classes": ["Class1", "Class2"],  // Maximum 3 classes, ordered by relevance
    "reasoning": [
        "1. Why this document primarily deals with Class1...",
        "2. Why Class2 is also relevant for this document...",
        "3. Why other classes are less relevant or not applicable..."
    ]
}

RULES:
- Consider the document as a whole, not individual columns.
- Select maximum 3 classes that best describe the document's domain and purpose.
- Base your decision on:
    * The overall content and purpose of the dataset.
    * The types of entities being described or linked.
    * The domain context suggested by the file name and data.
- Provide clear explanations for why these classes are most relevant.
- Explain why other classes were not selected.
- Only select classes that are strongly represented in the document.
"""
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o", # Can be made configurable
                messages=[
                    {"role": "system", "content": "You are a data scientist specialized in data classification. You always respond in the exact JSON format requested, with no additional text before or after."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                response_format={"type": "json_object"}
            )
            return json.loads(response.choices[0].message.content)
        except Exception as e:
            print(add_timestamp(color_text(f"Error getting classification for {file_info.get('file_name', '?')}: {e}", "red")))
            return None

    def prune_schema(self,
        data_lake_list: Union[list[dict], str],
        yaml_schema: Union[str, dict],
        output_directory: str | None = None,
    ) -> str | None:
        """
        Prunes the given LinkML schema (yaml_schema) to include only the classes
        that are relevant to the datasets in the data lake (data_lake_list).

        The pruning process involves the following steps:
        1. Classify each dataset in the data lake using the ChatGPT API and the
        class descriptions from the LinkML schema.
        2. Collect the relevant classes across all datasets.
        3. Create a new LinkML schema with only the relevant classes and their
        attributes.

        Args:
            data_lake_list (list[dict] or str): The list of dictionaries representing
                the data lake, or a path to a JSON file containing the data lake
                information.
            yaml_schema (str or dict): The LinkML schema as a dictionary, or a path
                to a YAML file containing the LinkML schema.
            output_directory (str, optional): The directory where the pruned schema
                should be saved. If not provided, the pruned schema is returned as a
                dictionary.

        Returns:
            str or None: The path to the pruned schema YAML file if output_directory
                is provided, otherwise the pruned schema as a dictionary.
        """
        if not data_lake_list:
            print(add_timestamp(color_text("No data lake information provided for schema pruning.", "yellow")))
            return None
        
        if isinstance(data_lake_list, str):
            if not os.path.exists(data_lake_list):
                print(add_timestamp(color_text(f"Error: Data lake JSON file not found at {data_lake_list}", "red")))
            data_lake_list = load_datalake_json(data_lake_list)
        
        if not yaml_schema:
            print(add_timestamp(color_text("No data lake information provided for schema pruning.", "yellow")))
            return None
        
        if isinstance(yaml_schema, str):
            if not os.path.exists(yaml_schema):
                print(add_timestamp(color_text(f"Error: Full YAML schema not found at {yaml_schema}", "red")))
                return None
            
            with open(yaml_schema, 'r', encoding='utf-8') as f:
                yaml_schema = yaml.safe_load(f)

        print(add_timestamp(color_text("Starting schema pruning...", "yellow")))

        # Load full schema descriptions (only top-level classes for pruning classification)
        _, full_yaml_descriptions = yaml_parser(yaml_schema)
        filtered_class_descriptions = {k: v for k, v in full_yaml_descriptions.items() if '.' not in k}

        # Collect relevant classes across all datasets
        all_relevant_classes = set()
        file_pruning_classifications = [] # To store detailed classifications per file

        for file_info in tqdm(data_lake_list, desc="Classifying datasets for pruning"):
            classification = self._classify_document_classes(file_info, filtered_class_descriptions)
            if classification and classification.get('relevant_classes'):
                all_relevant_classes.update(classification['relevant_classes'])
                file_pruning_classifications.append({
                    "file_name": file_info['file_name'],
                    "relevant_classes": classification['relevant_classes'],
                    "reasoning": classification.get('reasoning', [])
                })

        if not all_relevant_classes:
            print(add_timestamp(color_text("No relevant classes identified for any dataset. Pruned schema will be empty.", "yellow")))
            return None

        print(add_timestamp(color_text(f"Total unique relevant classes identified: {len(all_relevant_classes)}", "blue")))
        print(add_timestamp(color_text(f"Relevant classes: {list(all_relevant_classes)}", "blue")))

        pruned_schema_data = yaml_schema.copy()
        pruned_schema_data['classes'] = {}

        # Populate pruned schema with only the relevant classes and their attributes
        if 'classes' in yaml_schema:
            for class_name, class_details in yaml_schema['classes'].items():
                if class_name in all_relevant_classes:
                    pruned_schema_data['classes'][class_name] = class_details.copy()
                    # Ensure inherited classes are also included if they are not explicitly relevant
                    # This is a simplification; a full LinkML pruning would trace dependencies
                    is_a = class_details.get('is_a')
                    if is_a and is_a not in pruned_schema_data['classes'] and is_a in yaml_schema['classes']:
                        pruned_schema_data['classes'][is_a] = yaml_schema['classes'][is_a].copy()

        if not pruned_schema_data['classes']:
            print(add_timestamp(color_text("No classes left after pruning. Output YAML will be minimal.", "yellow")))

        # Save the pruned schema to a new YAML file
        if output_directory:
            try:
                create_directory_if_not_exists(output_directory)
                output_file = os.path.join(output_directory, "linkml_schema_pruned.yaml")
                with open(output_file, 'w', encoding='utf-8') as f:
                    yaml.dump(pruned_schema_data, f, indent=2, sort_keys=False)
                print(add_timestamp(color_text(f"Pruned LinkML schema saved to: {output_file}", "green")))

                
            except Exception as e:
                print(add_timestamp(color_text(f"Error saving pruned schema to {output_file}: {e}", "red")))
                return None
        
        return pruned_schema_data
