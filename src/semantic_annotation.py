import os
import sys
import pandas as pd
import json
import yaml
from tqdm import tqdm
from typing import Union
import copy # Import copy for deep copy
from utils import add_timestamp, color_text, yaml_parser, load_datalake_json, create_directory_if_not_exists

# Modify sys.path to include the archetype directory for local import
current_dir = os.path.dirname(os.path.abspath(__file__))
archetype_path = os.path.join(current_dir, 'archetype')
sys.path.insert(1, archetype_path)
try:
    from archetype.src.predict import ArcheTypePredictor
except ImportError:
    print(f"Error: Could not import ArcheTypePredictor from {archetype_path}. "
          "Please ensure the 'archetype' library is correctly installed and accessible "
          "at the specified path or in your Python environment.")
    ArcheTypePredictor = None # Set to None to handle gracefully

def archetype_annotation(
    data_lake_list: Union[list[dict], str],
    yaml_schema: Union[str, dict],
    sample_size: int = 20,
    output_directory: str | None = None
) -> list[dict]:
    """
    Annotates the given data lake representation with semantic annotations using the Archetype library.

    Args:
        data_lake_list (list[dict] | str): The list of dictionaries representing the data lake,
                                        as generated by DataLoader.load_and_describe_datalake.
        yaml_schema (str | dict): The LinkML schema as a dictionary, or a path to a YAML file containing the LinkML schema.
        sample_size (int): The number of rows to sample from each file for annotation.
        output_directory (str | None): If specified, the annotated data lake JSON will be saved to a file
                                    in this directory, named "data_lake_annotated.json".

    Returns:
        list[dict]: The annotated data lake representation.
    """
    if not data_lake_list:
        print(add_timestamp(color_text("No data lake information provided for schema pruning.", "yellow")))
        return None
        
    if isinstance(data_lake_list, str):
        if not os.path.exists(data_lake_list):
            print(add_timestamp(color_text(f"Error: Data lake JSON file not found at {data_lake_list}", "red")))
        data_lake_list = load_datalake_json(data_lake_list)
    
    if not yaml_schema:
        print(add_timestamp(color_text("No data lake information provided for schema pruning.", "yellow")))
        return None
    
    if isinstance(yaml_schema, str):
        if not os.path.exists(yaml_schema):
            print(add_timestamp(color_text(f"Error: Full YAML schema not found at {yaml_schema}", "red")))
            return None
        
        with open(yaml_schema, 'r', encoding='utf-8') as f:
            yaml_schema = yaml.safe_load(f)
    
    if ArcheTypePredictor is None:
        print(add_timestamp(color_text("ArchetypePredictor is not available, skipping annotation.", "red")))
        return copy.deepcopy(data_lake_list) # Return a copy without annotations

    if not data_lake_list:
        print(add_timestamp(color_text("No data lake information provided for semantic annotation.", "yellow")))
        return []

    print(add_timestamp(color_text("Starting semantic annotation with Archetype...", "magenta")))

    # Create a deep copy to avoid modifying the original input list
    annotated_data_lake_list = copy.deepcopy(data_lake_list)

    # Load full YAML schema labels and their descriptions
    all_labels, _ = yaml_parser(yaml_schema)
    archetype_labels = all_labels or ["NA"]

    print(add_timestamp(color_text(f"Loaded YAML schema from for Archetype.", "blue")))

    for file_info in tqdm(annotated_data_lake_list, desc="Annotating files"):
        file_name = file_info['file_name']
        file_path = file_info['file_path'] # Use the stored file_path to read original data

        if not os.path.exists(file_path):
            print(add_timestamp(color_text(f"Original data file not found at {file_path}, skipping annotation for {file_name}.", "red")))
            for col_dict in file_info['columns']:
                col_dict['semantic_annotation'] = "NA"
            continue

        sep = '\t' if file_name.endswith('.tsv') else ','

        print(add_timestamp(color_text(f"Processing file: {file_name}", "cyan")))

        # Archetype needs the actual DataFrame for sampling
        df_original = pd.read_csv(file_path, sep=sep, dtype=str, low_memory=False)
        if df_original.empty:
            print(add_timestamp(color_text(f"File {file_name} is empty, skipping annotation.", "yellow")))
            for col_dict in file_info['columns']:
                col_dict['semantic_annotation'] = "NA"
            continue

        # Archetype expects masked column names (e.g., Column_1, Column_2)
        original_column_names = df_original.columns.tolist()
        masked_columns = [f"Column_{i+1}" for i in range(len(original_column_names))]
        df_archetype_input = df_original.copy()
        df_archetype_input.columns = masked_columns

        sample_size = min(sample_size, len(df_archetype_input)) # Ensure sample size doesn't exceed dataframe size
        df_sample = df_archetype_input.sample(n=sample_size, random_state=42)

        # Archetype setup
        args = {
            "model_name": "gpt-4o", # Can be made configurable
            "custom_labels": archetype_labels,
            "summ_stats": True,
            "sample_size": sample_size
        }

        try:
            arch = ArcheTypePredictor(input_files=[df_sample], user_args=args)
            annotations_df = arch.annotate_columns()
            
            column_annotations = {
                f"Column_{i+1}": annotations_df.columns[i] 
                for i in range(len(annotations_df.columns))
            }

            # Map masked column names back to original column names and clean annotations
            for i, original_col_name in enumerate(original_column_names):
                masked_col_key = f"Column_{i+1}"
                annotation_label = column_annotations.get(masked_col_key, "")
                
                # If annotation contains "context" or is empty, set to "NA"
                if "context" in annotation_label.lower() or not annotation_label.strip():
                    file_info['columns'][i]['semantic_annotation'] = "NA"
                else:
                    file_info['columns'][i]['semantic_annotation'] = annotation_label.strip()

        except Exception as e:
            print(add_timestamp(color_text(f"Error running Archetype for {file_name}: {e}. Setting all annotations to 'NA'.", "red")))
            import traceback
            print(traceback.print_exc())
            for col_dict in file_info['columns']:
                col_dict['semantic_annotation'] = "NA"

    print(add_timestamp(color_text(f"Finished semantic annotation for {len(annotated_data_lake_list)} files.", "green")))
    
    if output_directory:
        try:
            create_directory_if_not_exists(output_directory)
            output_file = os.path.join(output_directory, "data_lake_annotated.json")
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(annotated_data_lake_list, f, indent=2)
            print(add_timestamp(color_text(f"Annotated data lake JSON saved to: {output_file}", "green")))
        except Exception as e:
            print(add_timestamp(color_text(f"Error saving annotated data lake JSON to {output_file}: {e}", "red")))
    
    return annotated_data_lake_list

